{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Opqrq4o_M7GehF-H63GeI6v2i4Av7-FH",
      "authorship_tag": "ABX9TyPPdPyLLXIjmX/+jIr5ZJLa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RutherfordOtu12/Application-of-GeoAI-A-Comparative-Methodical-urban-mapping-/blob/main/Validation_points_stratified.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from shapely.geometry import Point\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 1: LOAD DATA\n",
        "# ============================================================================\n",
        "print(\"=\" * 80)\n",
        "print(\"STEP 1: LOADING DATA\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Load the Friedrichshain-Kreuzberg land use data\n",
        "print(\"\\nLoading FK land use data...\")\n",
        "fk_landuse = gpd.read_file('/content/drive/MyDrive/FK_landuse_clipped.gpkg')  # Adjust path as needed\n",
        "print(f\"✓ Loaded {len(fk_landuse)} land use polygons\")\n",
        "print(f\"✓ CRS: {fk_landuse.crs}\")\n",
        "print(f\"✓ Columns: {list(fk_landuse.columns)}\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 2: DEFINE CLASSIFICATION MAPPING\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 2: DEFINING CLASSIFICATION MAPPING\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Mapping from official Berlin codes to research classes\n",
        "official_to_research = {\n",
        "    10: 1,      # Wohnnutzung → Residential\n",
        "    21: 2,      # Mischnutzung → Mixed-Use\n",
        "    30: 3,      # Kerngebietsnutzung → Commercial and Industrial/ Large-Scale Retail/Logistics\n",
        "    40: 3,      # Gewerbe- und Industrienutzung → Commercial and Industrial/ Large-Scale Retail/Logistics\n",
        "    50: 4,      # Gemeinbedarfs- und Sondernutzung → Public/Institutional\n",
        "    60: 3,      # Ver- und Entsorgung → Commercial and Industrial/ Large-Scale Retail/Logistics\n",
        "    70: 5,      # Wochenendhaus → Green Space and Recreational\n",
        "    80: 6,      # Verkehrsfläche → Transportation and infrastructure\n",
        "    100: 5,     # Wald → Green Space and Recreational\n",
        "    110: None,  # Gewässer → EXCLUDED\n",
        "    121: 5,     # Grünland → Green Space and Recreational\n",
        "    122: 5,     # Ackerland → Green Space and Recreational\n",
        "    130: 5,     # Park/Grünfläche → Green Space and Recreational\n",
        "    140: None,  # Stadtplatz/Promenade → NEEDS INSPECTION (set to None for now)\n",
        "    150: 5,     # Friedhof → Green Space and Recreational\n",
        "    160: 5,     # Kleingartenanlage → Green Space and Recreational\n",
        "    171: None,  # Brachfläche, vegetationsfrei → EXCLUDED\n",
        "    172: None,  # Brachfläche, wiesenartiger → EXCLUDED\n",
        "    173: None,  # Brachfläche, Mischbestand → EXCLUDED\n",
        "    200: 5,     # Baumschule/Gartenbau → Green Space and Recreational\n",
        "    90: None,   # Baustelle → EXCLUDED\n",
        "}\n",
        "\n",
        "# Class names for reference\n",
        "research_classes = {\n",
        "    1: \"Residential\",\n",
        "    2: \"Mixed-Use\",\n",
        "    3: \"Commercial and Industrial/ Large-Scale Retail/Logistics\",\n",
        "    4: \"Public/Institutional\",\n",
        "    5: \"Green Space and Recreational\",\n",
        "    6: \"Transportation and infrastructure\",\n",
        "}\n",
        "\n",
        "print(\"\\nClassification Mapping:\")\n",
        "print(\"-\" * 80)\n",
        "for official_code, research_class in sorted(official_to_research.items()):\n",
        "    if research_class is None:\n",
        "        status = \"EXCLUDED\"\n",
        "    else:\n",
        "        status = f\"→ Class {research_class}: {research_classes[research_class]}\"\n",
        "    print(f\"  Code {official_code:3d}: {status}\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 3: PREPARE DATA FOR SAMPLING\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 3: PREPARING DATA FOR SAMPLING\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Identify the column containing official codes\n",
        "# The column 'schluessel' was identified as the problem in previous run.\n",
        "# Based on the context of land use, 'nutz' is a more likely candidate for official codes.\n",
        "# Explicitly set code_column to 'nutz'\n",
        "code_column = 'nutz'\n",
        "\n",
        "if code_column not in fk_landuse.columns:\n",
        "    print(f\"\\nERROR: The expected code column '{code_column}' was not found!\")\n",
        "    print(f\"Available columns: {list(fk_landuse.columns)}\")\n",
        "    exit(1)\n",
        "\n",
        "print(f\"\\n✓ Using column '{code_column}' for official codes\")\n",
        "\n",
        "# Map official codes to research classes\n",
        "print(\"\\nMapping official codes to research classes...\")\n",
        "fk_landuse['official_code'] = fk_landuse[code_column].astype(int)\n",
        "\n",
        "# --- DEBUGGING: Inspect unique values and mapping results ---\n",
        "print(f\"\\nUnique values in '{code_column}' column before mapping:\")\n",
        "print(fk_landuse[code_column].value_counts().sort_index())\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "fk_landuse['research_class'] = fk_landuse['official_code'].map(official_to_research)\n",
        "\n",
        "# --- DEBUGGING: Inspect unique values and mapping results ---\n",
        "mapped_values_count = fk_landuse['research_class'].value_counts(dropna=False)\n",
        "print(\"\\nCounts of research_class values after mapping (including NaNs):\")\n",
        "print(mapped_values_count)\n",
        "\n",
        "non_mapped_codes = fk_landuse[fk_landuse['research_class'].isna()]['official_code'].unique()\n",
        "print(f\"\\nOfficial codes that resulted in NaN/None research_class: {sorted(non_mapped_codes.tolist())}\")\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "# Remove excluded classes\n",
        "fk_landuse_valid = fk_landuse[fk_landuse['research_class'].notna()].copy()\n",
        "print(f\"✓ Total polygons: {len(fk_landuse)}\")\n",
        "print(f\"✓ Valid polygons (after excluding): {len(fk_landuse_valid)}\")\n",
        "print(f\"✓ Excluded polygons: {len(fk_landuse) - len(fk_landuse_valid)}\")\n",
        "\n",
        "# Calculate polygon areas for weighted sampling\n",
        "fk_landuse_valid['area'] = fk_landuse_valid.geometry.area\n",
        "print(f\"✓ Total area: {fk_landuse_valid['area'].sum() / 1e6:.2f} km²\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 4: CALCULATE EXPECTED CLASS DISTRIBUTION\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 4: CALCULATING EXPECTED CLASS DISTRIBUTION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Group by research class and calculate statistics\n",
        "class_stats = fk_landuse_valid.groupby('research_class').agg({\n",
        "    'area': ['sum', 'mean', 'count'],\n",
        "    'geometry': 'count'\n",
        "}).round(2)\n",
        "\n",
        "print(\"\\nClass Statistics (by area):\")\n",
        "print(\"-\" * 80)\n",
        "total_area = fk_landuse_valid['area'].sum()\n",
        "for research_class in sorted(fk_landuse_valid['research_class'].unique()):\n",
        "    class_data = fk_landuse_valid[fk_landuse_valid['research_class'] == research_class]\n",
        "    class_area = class_data['area'].sum()\n",
        "    class_pct = (class_area / total_area) * 100\n",
        "    polygon_count = len(class_data)\n",
        "    print(f\"  Class {research_class}: {research_classes[research_class]}\")\n",
        "    print(f\"    - Area: {class_area / 1e6:.2f} km² ({class_pct:.1f}%)\")\n",
        "    print(f\"    - Polygons: {polygon_count}\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 5: STRATIFIED RANDOM POINT GENERATION\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 5: GENERATING STRATIFIED RANDOM VALIDATION POINTS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Target number of validation points\n",
        "target_points = 300\n",
        "\n",
        "# Expected distribution (from methodology)\n",
        "expected_distribution = {\n",
        "    1: 0.45,  # Residential: 40-50%\n",
        "    2: 0.10,  # Mixed-Use: 8-12%\n",
        "    3: 0.20,  # Commercial and Industrial/ Large-Scale Retail/Logistics: 15-25%\n",
        "    4: 0.075, # Public/Institutional: 5-10%\n",
        "    5: 0.125, # Green Space and Recreational: 10-15%\n",
        "    6: 0.075, # Transportation and infrastructure: 5-10%\n",
        "}\n",
        "\n",
        "# Calculate target points per class\n",
        "target_per_class = {}\n",
        "for research_class, pct in expected_distribution.items():\n",
        "    target_per_class[research_class] = int(target_points * pct)\n",
        "\n",
        "print(f\"\\nTarget total points: {target_points}\")\n",
        "print(\"\\nTarget distribution by class:\")\n",
        "print(\"-\" * 80)\n",
        "total_target = 0\n",
        "for research_class in sorted(target_per_class.keys()):\n",
        "    target = target_per_class[research_class]\n",
        "    pct = (target / target_points) * 100\n",
        "    print(f\"  Class {research_class}: {research_classes[research_class]}\")\n",
        "    print(f\"    - Target points: {target} ({pct:.1f}%)\")\n",
        "    total_target += target\n",
        "\n",
        "print(f\"\\nTotal target: {total_target}\")\n",
        "\n",
        "# Generate random points within each class\n",
        "validation_points = []\n",
        "validation_metadata = []\n",
        "\n",
        "print(\"\\nGenerating random points by class...\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "for research_class in sorted(fk_landuse_valid['research_class'].unique()):\n",
        "    class_data = fk_landuse_valid[fk_landuse_valid['research_class'] == research_class]\n",
        "    target = target_per_class.get(research_class, 0)\n",
        "\n",
        "    print(f\"\\n  Class {research_class}: {research_classes[research_class]}\")\n",
        "    print(f\"    - Generating {target} points from {len(class_data)} polygons...\")\n",
        "\n",
        "    # Generate random points\n",
        "    class_points = []\n",
        "    for idx, row in class_data.iterrows():\n",
        "        # Calculate expected number of points in this polygon\n",
        "        polygon_area = row['area']\n",
        "        class_area = class_data['area'].sum()\n",
        "        points_in_polygon = int(target * (polygon_area / class_area))\n",
        "\n",
        "        # Generate random points within polygon\n",
        "        for _ in range(points_in_polygon):\n",
        "            # Generate random point within polygon bounds\n",
        "            minx, miny, maxx, maxy = row['geometry'].bounds\n",
        "            while True:\n",
        "                x = np.random.uniform(minx, maxx)\n",
        "                y = np.random.uniform(miny, maxy)\n",
        "                point = Point(x, y)\n",
        "                if row['geometry'].contains(point):\n",
        "                    class_points.append(point)\n",
        "                    break\n",
        "\n",
        "    # If we need more points, add random ones\n",
        "    if len(class_points) < target:\n",
        "        for _ in range(target - len(class_points)):\n",
        "            random_polygon = class_data.sample(1).iloc[0]\n",
        "            minx, miny, maxx, maxy = random_polygon['geometry'].bounds\n",
        "            while True:\n",
        "                x = np.random.uniform(minx, maxx)\n",
        "                y = np.random.uniform(miny, maxy)\n",
        "                point = Point(x, y)\n",
        "                if random_polygon['geometry'].contains(point):\n",
        "                    class_points.append(point)\n",
        "                    break\n",
        "\n",
        "    # Trim to target if too many\n",
        "    class_points = class_points[:target]\n",
        "\n",
        "    print(f\"    - Generated: {len(class_points)} points\")\n",
        "\n",
        "    # Add to validation set\n",
        "    for i, point in enumerate(class_points):\n",
        "        validation_points.append(point)\n",
        "        validation_metadata.append({\n",
        "            'point_id': len(validation_points),\n",
        "            'research_class': research_class,\n",
        "            'class_name': research_classes[research_class],\n",
        "            'official_code': None,  # Will be assigned via spatial join\n",
        "        })\n",
        "\n",
        "print(f\"\\n✓ Total validation points generated: {len(validation_points)}\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 6: CREATE GEODATAFRAME AND ASSIGN OFFICIAL CODES\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 6: CREATING GEODATAFRAME AND ASSIGNING OFFICIAL CODES\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Create GeoDataFrame\n",
        "validation_gdf = gpd.GeoDataFrame(\n",
        "    validation_metadata,\n",
        "    geometry=validation_points,\n",
        "    crs=fk_landuse.crs\n",
        ")\n",
        "\n",
        "print(f\"\\n✓ Created GeoDataFrame with {len(validation_gdf)} points\")\n",
        "print(f\"✓ CRS: {validation_gdf.crs}\")\n",
        "\n",
        "# Spatial join to get official codes\n",
        "print(\"\\nPerforming spatial join to assign official codes...\")\n",
        "validation_gdf = gpd.sjoin(\n",
        "    validation_gdf,\n",
        "    fk_landuse[['official_code', 'geometry']],\n",
        "    how='left',\n",
        "    predicate='within'\n",
        ")\n",
        "\n",
        "# Rename the joined column\n",
        "validation_gdf = validation_gdf.rename(columns={'official_code_right': 'official_code_assigned'})\n",
        "validation_gdf = validation_gdf.drop(columns=['index_right'], errors='ignore')\n",
        "\n",
        "print(\"✓ Spatial join complete\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 7: QUALITY ASSURANCE\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 7: QUALITY ASSURANCE\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\nValidation Point Summary:\")\n",
        "print(\"-\" * 80)\n",
        "for research_class in sorted(validation_gdf['research_class'].unique()):\n",
        "    class_points = validation_gdf[validation_gdf['research_class'] == research_class]\n",
        "    print(f\"  Class {research_class}: {research_classes[research_class]}\")\n",
        "    print(f\"    - Points: {len(class_points)}\")\n",
        "    print(f\"    - Official codes assigned: {class_points['official_code_assigned'].notna().sum()}\")\n",
        "\n",
        "print(f\"\\n✓ Total validation points: {len(validation_gdf)}\")\n",
        "print(f\"✓ Points with official codes: {validation_gdf['official_code_assigned'].notna().sum()}\")\n",
        "print(f\"✓ Points without codes: {validation_gdf['official_code_assigned'].isna().sum()}\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 8: SAVE OUTPUTS\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 8: SAVING OUTPUTS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Save as GeoPackage\n",
        "output_gpkg = 'FK_validation_points_300.gpkg'\n",
        "validation_gdf.to_file(output_gpkg, layer='validation_points')\n",
        "print(f\"\\n✓ Saved to: {output_gpkg}\")\n",
        "\n",
        "# Save as CSV for spreadsheet use\n",
        "output_csv = 'FK_validation_points_300.csv'\n",
        "validation_csv = validation_gdf.copy()\n",
        "validation_csv['geometry'] = validation_csv['geometry'].astype(str)\n",
        "validation_csv.to_csv(output_csv, index=False)\n",
        "print(f\"✓ Saved to: {output_csv}\")\n",
        "\n",
        "# Save as Shapefile\n",
        "output_shp = 'FK_validation_points_300.shp'\n",
        "validation_gdf.to_file(output_shp)\n",
        "print(f\"✓ Saved to: {output_shp}\")\n",
        "\n",
        "# Create summary report\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 9: GENERATING SUMMARY REPORT\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "summary_report = f\"\"\"\n",
        "VALIDATION DATASET GENERATION REPORT\n",
        "=====================================\n",
        "Generated: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "\n",
        "DATASET SUMMARY\n",
        "---------------\n",
        "Total Validation Points: {len(validation_gdf)}\n",
        "CRS: {validation_gdf.crs}\n",
        "Study Area: Friedrichshain-Kreuzberg, Berlin\n",
        "\n",
        "CLASS DISTRIBUTION\n",
        "------------------\n",
        "\"\"\"\n",
        "\n",
        "for research_class in sorted(validation_gdf['research_class'].unique()):\n",
        "    class_points = validation_gdf[validation_gdf['research_class'] == research_class]\n",
        "    pct = (len(class_points) / len(validation_gdf)) * 100\n",
        "    summary_report += f\"\\nClass {research_class}: {research_classes[research_class]}\\n\"\n",
        "    summary_report += f\"  Points: {len(class_points)} ({pct:.1f}%)\\n\"\n",
        "    summary_report += f\"  Official codes assigned: {class_points['official_code_assigned'].notna().sum()}\\n\"\n",
        "\n",
        "summary_report += f\"\"\"\n",
        "\n",
        "OUTPUT FILES\n",
        "------------\n",
        "1. {output_gpkg} - GeoPackage format (recommended for GIS)\n",
        "2. {output_shp} - Shapefile format\n",
        "3. {output_csv} - CSV format (for spreadsheets)\n",
        "4. FK_validation_summary.txt - This report\n",
        "\n",
        "NEXT STEPS\n",
        "----------\n",
        "1. Review the validation points in QGIS\n",
        "2. Verify spatial distribution across study area\n",
        "3. Check for any points outside study area\n",
        "4. Prepare field collection materials\n",
        "5. Begin ground truth verification\n",
        "\n",
        "NOTES\n",
        "-----\n",
        "- Points were generated using stratified random sampling\n",
        "- Expected distribution was based on land use class proportions\n",
        "- Official codes assigned via spatial join with land use polygons\n",
        "- Some points may fall on boundaries; verify in field\n",
        "\"\"\"\n",
        "\n",
        "report_file = 'FK_validation_summary.txt'\n",
        "with open(report_file, 'w') as f:\n",
        "    f.write(summary_report)\n",
        "\n",
        "print(f\"✓ Saved summary report to: {report_file}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"VALIDATION POINT GENERATION COMPLETE!\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\nGenerated {len(validation_gdf)} validation points\")\n",
        "print(f\"Files saved:\")\n",
        "print(f\"  - {output_gpkg}\")\n",
        "print(f\"  - {output_shp}\")\n",
        "print(f\"  - {output_csv}\")\n",
        "print(f\"  - {report_file}\")"
      ],
      "metadata": {
        "id": "y-fD2qw2g_9z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ea2154b-437d-46c8-8bc7-68db608e7c08",
        "collapsed": true
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "STEP 1: LOADING DATA\n",
            "================================================================================\n",
            "\n",
            "Loading FK land use data...\n",
            "✓ Loaded 851 land use polygons\n",
            "✓ CRS: EPSG:25833\n",
            "✓ Columns: ['schluessel', 'flalle', 'bez', 'bezirk', 'woz', 'woz_name', 'ewoz_name', 'grz', 'grz_name', 'egrz_name', 'typ', 'typklar', 'etypklar', 'nutz', 'nutzung', 'enutzung', 'nutz_bauvor', 'nutzung_bauvor', 'enutzung_bauvor', 'ststrnr', 'ststrname', 'eststrname', 'vg_2021', 'probau_2021', 'provgneu_2021', 'vg_0_2021', 'provgneu_0_2021', 'kl1', 'kl2', 'kl3', 'kl4', 'geometry']\n",
            "\n",
            "================================================================================\n",
            "STEP 2: DEFINING CLASSIFICATION MAPPING\n",
            "================================================================================\n",
            "\n",
            "Classification Mapping:\n",
            "--------------------------------------------------------------------------------\n",
            "  Code  10: → Class 1: Residential\n",
            "  Code  21: → Class 2: Mixed-Use\n",
            "  Code  30: → Class 3: Commercial and Industrial/ Large-Scale Retail/Logistics\n",
            "  Code  40: → Class 3: Commercial and Industrial/ Large-Scale Retail/Logistics\n",
            "  Code  50: → Class 4: Public/Institutional\n",
            "  Code  60: → Class 3: Commercial and Industrial/ Large-Scale Retail/Logistics\n",
            "  Code  70: → Class 5: Green Space and Recreational\n",
            "  Code  80: → Class 6: Transportation and infrastructure\n",
            "  Code  90: EXCLUDED\n",
            "  Code 100: → Class 5: Green Space and Recreational\n",
            "  Code 110: EXCLUDED\n",
            "  Code 121: → Class 5: Green Space and Recreational\n",
            "  Code 122: → Class 5: Green Space and Recreational\n",
            "  Code 130: → Class 5: Green Space and Recreational\n",
            "  Code 140: EXCLUDED\n",
            "  Code 150: → Class 5: Green Space and Recreational\n",
            "  Code 160: → Class 5: Green Space and Recreational\n",
            "  Code 171: EXCLUDED\n",
            "  Code 172: EXCLUDED\n",
            "  Code 173: EXCLUDED\n",
            "  Code 200: → Class 5: Green Space and Recreational\n",
            "\n",
            "================================================================================\n",
            "STEP 3: PREPARING DATA FOR SAMPLING\n",
            "================================================================================\n",
            "\n",
            "✓ Using column 'nutz' for official codes\n",
            "\n",
            "Mapping official codes to research classes...\n",
            "\n",
            "Unique values in 'nutz' column before mapping:\n",
            "nutz\n",
            "10     335\n",
            "21      84\n",
            "30      15\n",
            "40      65\n",
            "50      84\n",
            "60       6\n",
            "80      38\n",
            "90       1\n",
            "100      2\n",
            "110     31\n",
            "130    126\n",
            "140     19\n",
            "150      8\n",
            "160      3\n",
            "171      2\n",
            "172      3\n",
            "173      7\n",
            "190     21\n",
            "200      1\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Counts of research_class values after mapping (including NaNs):\n",
            "research_class\n",
            "1.0    335\n",
            "5.0    140\n",
            "3.0     86\n",
            "2.0     84\n",
            "NaN     84\n",
            "4.0     84\n",
            "6.0     38\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Official codes that resulted in NaN/None research_class: [90, 110, 140, 171, 172, 173, 190]\n",
            "✓ Total polygons: 851\n",
            "✓ Valid polygons (after excluding): 767\n",
            "✓ Excluded polygons: 84\n",
            "✓ Total area: 14.11 km²\n",
            "\n",
            "================================================================================\n",
            "STEP 4: CALCULATING EXPECTED CLASS DISTRIBUTION\n",
            "================================================================================\n",
            "\n",
            "Class Statistics (by area):\n",
            "--------------------------------------------------------------------------------\n",
            "  Class 1.0: Residential\n",
            "    - Area: 5.68 km² (40.2%)\n",
            "    - Polygons: 335\n",
            "  Class 2.0: Mixed-Use\n",
            "    - Area: 2.01 km² (14.2%)\n",
            "    - Polygons: 84\n",
            "  Class 3.0: Commercial and Industrial/ Large-Scale Retail/Logistics\n",
            "    - Area: 1.70 km² (12.1%)\n",
            "    - Polygons: 86\n",
            "  Class 4.0: Public/Institutional\n",
            "    - Area: 1.77 km² (12.6%)\n",
            "    - Polygons: 84\n",
            "  Class 5.0: Green Space and Recreational\n",
            "    - Area: 2.42 km² (17.2%)\n",
            "    - Polygons: 140\n",
            "  Class 6.0: Transportation and infrastructure\n",
            "    - Area: 0.53 km² (3.7%)\n",
            "    - Polygons: 38\n",
            "\n",
            "================================================================================\n",
            "STEP 5: GENERATING STRATIFIED RANDOM VALIDATION POINTS\n",
            "================================================================================\n",
            "\n",
            "Target total points: 300\n",
            "\n",
            "Target distribution by class:\n",
            "--------------------------------------------------------------------------------\n",
            "  Class 1: Residential\n",
            "    - Target points: 135 (45.0%)\n",
            "  Class 2: Mixed-Use\n",
            "    - Target points: 30 (10.0%)\n",
            "  Class 3: Commercial and Industrial/ Large-Scale Retail/Logistics\n",
            "    - Target points: 60 (20.0%)\n",
            "  Class 4: Public/Institutional\n",
            "    - Target points: 22 (7.3%)\n",
            "  Class 5: Green Space and Recreational\n",
            "    - Target points: 37 (12.3%)\n",
            "  Class 6: Transportation and infrastructure\n",
            "    - Target points: 22 (7.3%)\n",
            "\n",
            "Total target: 306\n",
            "\n",
            "Generating random points by class...\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "  Class 1.0: Residential\n",
            "    - Generating 135 points from 335 polygons...\n",
            "    - Generated: 135 points\n",
            "\n",
            "  Class 2.0: Mixed-Use\n",
            "    - Generating 30 points from 84 polygons...\n",
            "    - Generated: 30 points\n",
            "\n",
            "  Class 3.0: Commercial and Industrial/ Large-Scale Retail/Logistics\n",
            "    - Generating 60 points from 86 polygons...\n",
            "    - Generated: 60 points\n",
            "\n",
            "  Class 4.0: Public/Institutional\n",
            "    - Generating 22 points from 84 polygons...\n",
            "    - Generated: 22 points\n",
            "\n",
            "  Class 5.0: Green Space and Recreational\n",
            "    - Generating 37 points from 140 polygons...\n",
            "    - Generated: 37 points\n",
            "\n",
            "  Class 6.0: Transportation and infrastructure\n",
            "    - Generating 22 points from 38 polygons...\n",
            "    - Generated: 22 points\n",
            "\n",
            "✓ Total validation points generated: 306\n",
            "\n",
            "================================================================================\n",
            "STEP 6: CREATING GEODATAFRAME AND ASSIGNING OFFICIAL CODES\n",
            "================================================================================\n",
            "\n",
            "✓ Created GeoDataFrame with 306 points\n",
            "✓ CRS: EPSG:25833\n",
            "\n",
            "Performing spatial join to assign official codes...\n",
            "✓ Spatial join complete\n",
            "\n",
            "================================================================================\n",
            "STEP 7: QUALITY ASSURANCE\n",
            "================================================================================\n",
            "\n",
            "Validation Point Summary:\n",
            "--------------------------------------------------------------------------------\n",
            "  Class 1.0: Residential\n",
            "    - Points: 135\n",
            "    - Official codes assigned: 135\n",
            "  Class 2.0: Mixed-Use\n",
            "    - Points: 30\n",
            "    - Official codes assigned: 30\n",
            "  Class 3.0: Commercial and Industrial/ Large-Scale Retail/Logistics\n",
            "    - Points: 60\n",
            "    - Official codes assigned: 60\n",
            "  Class 4.0: Public/Institutional\n",
            "    - Points: 22\n",
            "    - Official codes assigned: 22\n",
            "  Class 5.0: Green Space and Recreational\n",
            "    - Points: 37\n",
            "    - Official codes assigned: 37\n",
            "  Class 6.0: Transportation and infrastructure\n",
            "    - Points: 22\n",
            "    - Official codes assigned: 22\n",
            "\n",
            "✓ Total validation points: 306\n",
            "✓ Points with official codes: 306\n",
            "✓ Points without codes: 0\n",
            "\n",
            "================================================================================\n",
            "STEP 8: SAVING OUTPUTS\n",
            "================================================================================\n",
            "\n",
            "✓ Saved to: FK_validation_points_300.gpkg\n",
            "✓ Saved to: FK_validation_points_300.csv\n",
            "✓ Saved to: FK_validation_points_300.shp\n",
            "\n",
            "================================================================================\n",
            "STEP 9: GENERATING SUMMARY REPORT\n",
            "================================================================================\n",
            "✓ Saved summary report to: FK_validation_summary.txt\n",
            "\n",
            "================================================================================\n",
            "VALIDATION POINT GENERATION COMPLETE!\n",
            "================================================================================\n",
            "\n",
            "Generated 306 validation points\n",
            "Files saved:\n",
            "  - FK_validation_points_300.gpkg\n",
            "  - FK_validation_points_300.shp\n",
            "  - FK_validation_points_300.csv\n",
            "  - FK_validation_summary.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7U8bsQ5WQQ9b"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}